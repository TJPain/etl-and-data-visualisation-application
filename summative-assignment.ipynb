{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9169d50a",
   "metadata": {},
   "source": [
    "# Advanced Programming Summative Assignment\n",
    "\n",
    "Please see the README file (README.md) for a full overview of the project, installation instructions and running instructions. \n",
    "\n",
    "## Contents\n",
    "\n",
    "#### 1.0 [Package installations and import statements](#1.0-Package-installations-and-import-statements)\n",
    "\n",
    "#### 2.0 [Data extraction and cleaning](#2.0-Data-extraction-and-cleaning)\n",
    "\n",
    "2.1 [Read CSVs to dataframes and general data cleaning](#2.2-Merge-airport-and-frequency-dataframes)\n",
    "\n",
    "2.2 [Merge airport and frequency dataframes](#2.1-Read-CSVs-to-dataframes-and-general-data-cleaning)\n",
    "\n",
    "2.3 [Transform dataframes to JSON](#2.3-Transform-dataframes-to-JSON)\n",
    "\n",
    "#### 3.0 [Load data to MySQL database](#3.0-Load-data-to-MySQL-database)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ac444f",
   "metadata": {},
   "source": [
    "### 1.0 Package installations and import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38ca82a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - pandastable\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing missing packages\n",
    "# import sys\n",
    "%conda install --yes --prefix {sys.prefix} numpy\n",
    "%conda install --yes --prefix {sys.prefix} pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1453cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "# import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pprint\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15799b3",
   "metadata": {},
   "source": [
    "### 2.0 Data extraction and cleaning\n",
    "\n",
    "A lot of the data processing is generic and not specific to the original data. This allows the application to handle other datasets from the same source. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5457f",
   "metadata": {},
   "source": [
    "#### 2.1 Read CSVs to dataframes and general data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11e41857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_date(): \n",
    "    # Extract data from CSV files\n",
    "    df_runways = pd.read_csv ('data/runways.csv', index_col=['id'])\n",
    "    df_frequencies = pd.read_csv ('data/airport-frequencies.csv', index_col=['id'])\n",
    "    # Prevent pandas from replacing the 'continent' value 'NA' (for North America) with NaN \n",
    "    df_airports = pd.read_csv ('data/airports.csv', keep_default_na=False)\n",
    "    return (df_runways, df_frequencies, df_airports)\n",
    "\n",
    "def remove_missing_data(df_runways, df_frequencies, df_airports):\n",
    "    # Remove any rows with all data missing\n",
    "    df_airports.dropna(how='all')\n",
    "    df_runways.dropna(how='all')\n",
    "    df_frequencies.dropna(how='all')\n",
    "\n",
    "def remove_duplicates(df_runways, df_frequencies, df_airports):\n",
    "    # Remove any duplicated rows\n",
    "    df_airports.drop_duplicates()\n",
    "    df_runways.drop_duplicates()\n",
    "    df_frequencies.drop_duplicates()\n",
    "\n",
    "def remove_cols(df_runways, df_frequencies, df_airports):\n",
    "    # Remove unneeded columns\n",
    "    df_airports.drop(['keywords','home_link','local_code'], axis='columns', inplace=True)\n",
    "    df_runways.drop(['airport_ident'], axis='columns', inplace=True)\n",
    "    df_frequencies.drop(['airport_ident','description','type'], axis='columns', inplace=True)\n",
    "\n",
    "def change_col_names(df_airports):\n",
    "    # change the airport column id name to airport_ref to align with other data\n",
    "    df_airports.rename(columns={\"id\": \"airport_ref\"}, inplace=True)\n",
    "\n",
    "def remove_invalid(df_runways, df_frequencies, df_airports):\n",
    "    # Remove rows that do not have a valid airport_ref\n",
    "    df_airports = df_airports[df_airports['airport_ref'].apply(lambda x: str(x).isdigit())]\n",
    "    df_runways = df_runways[df_runways['airport_ref'].apply(lambda x: str(x).isdigit())]\n",
    "    df_frequencies = df_frequencies[df_frequencies['airport_ref'].apply(lambda x: str(x).isdigit())]\n",
    "    return (df_runways, df_frequencies, df_airports)\n",
    "\n",
    "def add_new_cols(df_airports):\n",
    "    # Add columns to the Airports df for small, medium and large airports with binary values\n",
    "    df_airports['small_airport'] = df_airports.type == 'small_airport'\n",
    "    df_airports['medium_airport'] = df_airports.type == 'medium_airport'\n",
    "    df_airports['large_airport'] = df_airports.type == 'large_airport'\n",
    "    df_airports['small_airport'] = df_airports['small_airport'].astype(int) \n",
    "    df_airports['medium_airport'] = df_airports['medium_airport'].astype(int) \n",
    "    df_airports['large_airport'] = df_airports['large_airport'].astype(int) \n",
    "    return(df_airports)\n",
    "\n",
    "def remove_closed(df_airports):\n",
    "    # filter out closed airports - may need to force a copy, not sure yet - may need to do this after frequencies are added\n",
    "    df_airports = df_airports[(df_airports.type != 'closed')]\n",
    "    return(df_airports)\n",
    "\n",
    "def load_new_data():\n",
    "    df_runways, df_frequencies, df_airports = load_date()\n",
    "    remove_missing_data(df_runways, df_frequencies, df_airports)\n",
    "    remove_duplicates(df_runways, df_frequencies, df_airports)\n",
    "    remove_cols(df_runways, df_frequencies, df_airports)\n",
    "    change_col_names(df_airports)\n",
    "    remove_invalid(df_runways, df_frequencies, df_airports)\n",
    "    add_new_cols(df_airports)\n",
    "    remove_closed(df_airports)\n",
    "    return df_runways, df_frequencies, df_airports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c29c4fc",
   "metadata": {},
   "source": [
    "#### 2.2 Merge airport and frequency dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44a1a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequencies_to_list(df_frequencies):\n",
    "    # Create an empty dict to hold one key for each airport with a nested list of frequencies \n",
    "    frequencies_dict = {}\n",
    "\n",
    "    # Iterate through the frequencies creating one key for each airport with a list for it's frequencies \n",
    "    for index, row in df_frequencies.iterrows():\n",
    "        if row['airport_ref'] not in frequencies_dict:\n",
    "            frequencies_dict[row['airport_ref']] = [row['frequency_mhz']]\n",
    "        else:\n",
    "            frequencies_dict[row['airport_ref']].append(row['frequency_mhz'])\n",
    "    return frequencies_dict\n",
    "\n",
    "def merge_dataframes(frequencies_dict, df_airports):\n",
    "    # Create a pandas series of airports (as index) and frequency lists \n",
    "    df_frequencies_series = pd.Series(frequencies_dict, name='df_frequencies_series')\n",
    "    # Rename the column titles to align with other data \n",
    "    df_airports_frequencies = df_frequencies_series.to_frame()\n",
    "    df_airports_frequencies.index.name = 'airport_ref'\n",
    "    # Convert to dataframe and merge with airports dataframe \n",
    "    df_airports_frequencies.rename(columns={'df_frequencies_series': 'frequency_mhz'}, inplace = True)\n",
    "    df_airports_frequencies = pd.merge(df_airports, df_airports_frequencies, on=\"airport_ref\", how = 'left')\n",
    "    return df_airports_frequencies\n",
    "\n",
    "def merge_airports_frequencies(df_frequencies, df_airports):\n",
    "    frequencies_dict = frequencies_to_list(df_frequencies)\n",
    "    df_airports_frequencies = merge_dataframes(frequencies_dict, df_airports)\n",
    "    return df_airports_frequencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbfbec",
   "metadata": {},
   "source": [
    "#### 2.3 Transform dataframes to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05fb3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_json(df_airports_frequencies, df_runways):\n",
    "    # write combined airport and frequency data to JSON\n",
    "    airports_frequencies_json = df_airports_frequencies.to_json(orient = 'records')\n",
    "    airports_frequencies_json_list = json.loads(airports_frequencies_json)\n",
    "    # write runways data to JSON\n",
    "    runways_json = df_runways.to_json(orient = 'records')\n",
    "    runways_json_list = json.loads(runways_json)\n",
    "    return airports_frequencies_json_list, runways_json_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60fd2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(airports_frequencies_json_list, runways_json_list):\n",
    "    with open('./saved-data/airports_frequencies.json', 'w') as outfile:\n",
    "        json.dump(airports_frequencies_json_list, outfile)\n",
    "    with open('./saved-data/runways_json_list.json', 'w') as outfile:\n",
    "        json.dump(runways_json_list, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91e3ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open('./saved-data/airports_frequencies.json', 'r') as infile:\n",
    "        loaded_airports_frequencies = json.load(infile)\n",
    "    with open('./saved-data/runways_json_list.json', 'r') as infile:\n",
    "        loaded_runways = json.load(infile)\n",
    "    return loaded_airports_frequencies, loaded_runways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d65de7",
   "metadata": {},
   "source": [
    "### 3.0 Load data to MySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb169edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airport_ref': 323361,\n",
      " 'continent': 'NA',\n",
      " 'elevation_ft': '3435',\n",
      " 'frequency_mhz': None,\n",
      " 'gps_code': '00AA',\n",
      " 'iata_code': '',\n",
      " 'ident': '00AA',\n",
      " 'iso_country': 'US',\n",
      " 'iso_region': 'US-KS',\n",
      " 'large_airport': 0,\n",
      " 'latitude_deg': 38.704022,\n",
      " 'longitude_deg': -101.473911,\n",
      " 'medium_airport': 0,\n",
      " 'municipality': 'Leoti',\n",
      " 'name': 'Aero B Ranch Airport',\n",
      " 'scheduled_service': 'no',\n",
      " 'small_airport': 1,\n",
      " 'type': 'small_airport',\n",
      " 'wikipedia_link': ''}\n",
      "<class 'dict'>\n",
      " \n",
      "{'airport_ref': 323361,\n",
      " 'continent': 'NA',\n",
      " 'elevation_ft': '3435',\n",
      " 'frequency_mhz': None,\n",
      " 'gps_code': '00AA',\n",
      " 'iata_code': '',\n",
      " 'ident': '00AA',\n",
      " 'iso_country': 'US',\n",
      " 'iso_region': 'US-KS',\n",
      " 'large_airport': 0,\n",
      " 'latitude_deg': 38.704022,\n",
      " 'longitude_deg': -101.473911,\n",
      " 'medium_airport': 0,\n",
      " 'municipality': 'Leoti',\n",
      " 'name': 'Aero B Ranch Airport',\n",
      " 'scheduled_service': 'no',\n",
      " 'small_airport': 1,\n",
      " 'type': 'small_airport',\n",
      " 'wikipedia_link': ''}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# GUI code\n",
    "\n",
    "df_runways, df_frequencies, df_airports = load_new_data()\n",
    "df_airports_frequencies = merge_airports_frequencies(df_frequencies, df_airports)\n",
    "airports_frequencies_json_list, runways_json_list = transform_to_json(df_airports_frequencies, df_runways)\n",
    "save_data(airports_frequencies_json_list, runways_json_list)\n",
    "loaded_airports_frequencies, loaded_runways = load_data()\n",
    "# pprint.pprint(loaded_airports_frequencies[1])\n",
    "# print(type(loaded_airports_frequencies[1]))\n",
    "# print(\" \")\n",
    "# pprint.pprint(airports_frequencies_json_list[1])\n",
    "# print(type(airports_frequencies_json_list[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
